[{"path":"https://docs.ropensci.org/jstor/CONDUCT.html","id":null,"dir":"","previous_headings":"","what":"Contributor Code of Conduct","title":"Contributor Code of Conduct","text":"contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (http://contributor-covenant.org), version 1.0.0, available http://contributor-covenant.org/version/1/0/0/","code":""},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"general-concept","dir":"Articles","previous_headings":"","what":"General Concept","title":"Introduction to jstor","text":"functions jst_get_* family concerned meta data operate along lines: file read xml2::read_xml(). Content file extracted via XPATH CSS-expressions. resulting data returned tidy tibble. functions similar operate single files (article, book, research report pamphlet). Depending content file, output functions might one multiple rows. jst_get_article always returns tibble one row: core meta data (like title, id, first page article) single items, one article processed time. Running jst_get_authors article might give tibble one multiple rows, depending number authors article . true jst_get_references jst_get_footnotes. file data references (might still exist, JSTOR might parsed ), output one row, missing references. data references, entry gets row. Note however, number rows equal number references. References usually start title like “References”, obviously reference another article. sure think carefully assumptions check content data make inferences. Books work bit differently. Searching data https://www.jstor.org/dfr/results lets filter books, actually book chapters. receive data DfR book chapter, always get one xml-file whole book, including data chapters. Ngram full-text data entry however processed single chapters1. Thus, output jst_get_book single file similar one jst_get_article: one row general data book. jst_get_chapters gives data chapters, resulting tibble therefore might multiple rows. following sections showcase different functions separately.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"application","dir":"Articles","previous_headings":"","what":"Application","title":"Introduction to jstor","text":"Apart jstor need load dplyr matching records knitr printing nice tables.","code":"library(jstor) library(dplyr) library(knitr)"},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"jst_get_article","dir":"Articles","previous_headings":"Application","what":"jst_get_article","title":"Introduction to jstor","text":"basic usage jst_get_* functions simple. take one argument, path file import: resulting object tibble one row 17 columns. columns correspond elements documented : https://www.jstor.org/dfr//technical-specifications. columns : file_name (chr): file name original .xml-file. Can used joining parts (authors, references, footnotes, full-texts). journal_doi (chr): registered identifier journal. journal_jcode (chr): identifier journal like “amerjsoci” “American Journal Sociology”. journal_pub_id (chr): Similar journal_jcode. time either one present. article_doi (chr): registered unique identifier article. article_jcode (chr): unique identifier article (DOI). article_pub_id (chr): Infrequent, either part DOI article_jcode. article_type (chr): type article (research-article, book-review, etc.). article_title (chr): title article. volume (chr): volume article published . issue (chr): issue article published . language (chr): language article. pub_day (chr): Publication day, specified. pub_month (chr): Publication month, specified. pub_year (int): Year publication. first_page (int): Page number first page article. last_page (int): Page number last page article. Since output functions tibbles, result nicely formatted:","code":"meta_data <- jst_get_article(file_path = jst_example(\"article_with_references.xml\")) meta_data %>% kable()"},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"jst_get_authors","dir":"Articles","previous_headings":"Application","what":"jst_get_authors","title":"Introduction to jstor","text":"Extracting authors works similar fashion: following columns: file_name: , used matching articles. prefix: prefix name. given_name: given name author (.e. Albert .). surname: surname author (.e. Einstein). string_name: Sometimes instead given_name surname, full string supplied, .e.: Albert Einstein, Einstein, Albert. suffix: suffix name, Albert Einstein, II.. author_number: integer representing order authors appeared data. number rows matches number authors – author get ’ row.","code":"authors <- jst_get_authors(jst_example(\"article_with_references.xml\")) kable(authors)"},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"jst_get_references","dir":"Articles","previous_headings":"Application","what":"jst_get_references","title":"Introduction to jstor","text":"two columns: file_name: Identifier, can used matching. ref_title: title references sections. ref_authors: string authors. Several authors separated ;. ref_editors: string editors, present. ref_collab: field may contain information authors, authors available. ref_item_title: title cited entry. ref_year: year, often article’s publication year, always. ref_source: source cited entry. books often title book, articles publisher journal. ref_volume: volume journal article. ref_first_page: first page article/chapter. ref_last_page: last page article/chapter. ref_publisher: books publisher, articles often missing. ref_publication_type: Known types: book, journal, web, . ref_unparsed: full references entry unparsed form. display 5 random entries: example shows several things: file_name identical among rows, since identifies article references came one article. sample file doesn’t follow typical convention (published 1922), therefore several different headings (ref_title). Usually, “Bibliography” “References”. Since references parsed JSTOR, get unparsed version. general, content references (unparsed_refs) quite raw state, quite often result digitising scans via OCR. example, last entry reads like : MACHADO, .1911 Zytologische Untersuchungen fiber Trypanosoma rotatorium .... error : fiber über. language source German, OCR-software assumed English. Therefore, didn’t recognize Umlaut. Similar errors common text read via OCR. files, can set parse_refs = TRUE, references imported parsed form, whenever available. Note, might content present like endnotes, case article used endnotes rather footnotes.","code":"references <- jst_get_references(jst_example(\"article_with_references.xml\"))  # # we need to remove line breaks for knitr::kable() to work properly for printing references <- references %>%   mutate(ref_unparsed = stringr::str_remove_all(ref_unparsed, \"\\\\\\n\")) references %>%    sample_n(5) %>%    kable() jst_get_references(   jst_example(\"parsed_references.xml\"),   parse_refs = TRUE ) %>%    kable()"},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"jst_get_footnotes","dir":"Articles","previous_headings":"Application","what":"jst_get_footnotes","title":"Introduction to jstor","text":"commonly, articles either footnotes references. sample file used footnotes, therefore simple tibble missing footnotes returned. use another file demonstrate footnotes. general, might need combine jst_get_footnotes() jst_get_references() get available information citation data.","code":"jst_get_footnotes(jst_example(\"article_with_references.xml\")) %>%    kable() footnotes <- jst_get_footnotes(jst_example(\"article_with_footnotes.xml\"))  footnotes %>%    mutate(footnotes = stringr::str_remove_all(footnotes, \"\\\\\\n\")) %>%    kable()"},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"jst_get_full_text","dir":"Articles","previous_headings":"Application","what":"jst_get_full_text","title":"Introduction to jstor","text":"function extract full texts can’t demonstrated proper data, since full texts supplied upon special request DfR. function guesses encoding specified file via readr::guess_encoding(), reads whole file returns tibble file_name, full_text encoding. created file looks similar files supplied DfR sample text:","code":"full_text <- jst_get_full_text(jst_example(\"full_text.txt\")) full_text %>%    mutate(full_text = stringr::str_remove_all(full_text, \"\\\\\\n\")) %>%    kable()"},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"combining-results","dir":"Articles","previous_headings":"Application","what":"Combining results","title":"Introduction to jstor","text":"Different parts meta-data can combined using dplyr::left_join().","code":""},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"matching-with-authors","dir":"Articles","previous_headings":"Application > Combining results","what":"Matching with authors","title":"Introduction to jstor","text":"","code":"meta_data %>%    left_join(authors) %>%   select(file_name, article_title, pub_year, given_name, surname) %>%    kable() #> Joining with `by = join_by(file_name)`"},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"matching-with-references","dir":"Articles","previous_headings":"Application > Combining results","what":"Matching with references","title":"Introduction to jstor","text":"","code":"meta_data %>%    left_join(references) %>%    select(file_name, article_title, volume, pub_year, ref_unparsed) %>%   head(5) %>%    kable() #> Joining with `by = join_by(file_name)`"},{"path":"https://docs.ropensci.org/jstor/articles/Introduction.html","id":"books","dir":"Articles","previous_headings":"","what":"Books","title":"Introduction to jstor","text":"Quite recently DfR added book chapters stack. import metadata books chapters, jstor supplies jst_get_book jst_get_chapters. jst_get_book similar jst_get_article. obtain general information complete book: single book might contain many chapters. jst_get_chapters extracts . Due , function bit slower jstor’s functions. Without abstracts (rather long) first 10 chapters look like : Since extracting authors chapters needs considerably time, default authors extracted. can import like : authors supplied list column: can expand list tidyr::unnest: can learn concept list-columns Hadley Wickham’s book R Data Science.","code":"jst_get_book(jst_example(\"book.xml\")) %>% knitr::kable() chapters <- jst_get_chapters(jst_example(\"book.xml\"))  str(chapters) #> tibble [36 × 9] (S3: tbl_df/tbl/data.frame) #>  $ book_id        : chr [1:36] \"j.ctt24hdz7\" \"j.ctt24hdz7\" \"j.ctt24hdz7\" \"j.ctt24hdz7\" ... #>  $ file_name      : chr [1:36] \"book\" \"book\" \"book\" \"book\" ... #>  $ part_id        : chr [1:36] \"j.ctt24hdz7.1\" \"j.ctt24hdz7.2\" \"j.ctt24hdz7.3\" \"j.ctt24hdz7.4\" ... #>  $ part_label     : chr [1:36] NA NA NA NA ... #>  $ part_title     : chr [1:36] \"Front Matter\" \"Table of Contents\" \"Acronyms and abbreviations\" \"Authors’ biographies\" ... #>  $ part_subtitle  : chr [1:36] NA NA NA NA ... #>  $ authors        : chr [1:36] NA NA NA NA ... #>  $ abstract       : chr [1:36] NA NA NA NA ... #>  $ part_first_page: chr [1:36] \"i\" \"v\" \"vii\" \"xi\" ... chapters %>%    select(-abstract) %>%    head(10) %>%    kable() author_chap <- jst_get_chapters(jst_example(\"book.xml\"), authors = TRUE) class(author_chap$authors) #> [1] \"list\" author_chap %>%    tidyr::unnest(authors) %>%    select(part_id, given_name, surname) %>%    head(10) %>%    kable()"},{"path":"https://docs.ropensci.org/jstor/articles/analysing-n-grams.html","id":"cleaning-the-data","dir":"Articles","previous_headings":"","what":"Cleaning the data","title":"Analysing n-grams with `jstor` for R","text":"Data DfR inherently messy. fix common issues, can use jst_augment(): information common quirks data DfR deal , take look vignette(\"known-quirks\").","code":"imported_metadata <- jst_augment(imported_metadata, quietly = TRUE)"},{"path":"https://docs.ropensci.org/jstor/articles/analysing-n-grams.html","id":"exploration","dir":"Articles","previous_headings":"","what":"Exploration","title":"Analysing n-grams with `jstor` for R","text":"diving analysis n-grams, might wish take exploratory look metadata. first thing look types articles. plot chunk article-types can see, majority articles proper “research-articles”, together book-reviews miscellaneous articles amount ~99% articles. must cautious, however, using variable distinguish articles categories. instance, “research-articles” actually book-reviews: current demonstration, want restrict type articles research articles, therefore need take steps remove book reviews miscellaneous articles: First, filter article_type, remove articles title starts “Book Review”.","code":"ggplot(imported_metadata, aes(article_type)) +   geom_bar() +   coord_flip() imported_metadata %>%    count(article_type, sort = T) %>%    mutate(perc = scales::percent(n/sum(n))) ## # A tibble: 14 x 3 ##    article_type         n perc  ##    <chr>            <int> <chr> ##  1 research-article 16289 68.1% ##  2 book-review       4552 19.0% ##  3 misc              2850 11.9% ##  4 other               89 0.4%  ##  5 in-brief            38 0.2%  ##  6 discussion          31 0.1%  ##  7 review-article      25 0.1%  ##  8 announcement        12 0.1%  ##  9 index                9 0.0%  ## 10 editorial            7 0.0%  ## 11 introduction         3 0.0%  ## 12 news                 2 0.0%  ## 13 bibliography         1 0.0%  ## 14 letter               1 0.0% imported_metadata %>%    filter(article_type == \"research-article\" & str_detect(article_title, \"Book\")) %>%    select(file_name, article_title, pub_year) ## # A tibble: 190 x 3 ##    file_name                      article_title pub_year ##    <chr>                          <chr>            <int> ##  1 journal-article-10.1086_210272 Book Reviews      1999 ##  2 journal-article-10.1086_210273 Book Reviews      1999 ##  3 journal-article-10.1086_210274 Book Reviews      1999 ##  4 journal-article-10.1086_210275 Book Reviews      1999 ##  5 journal-article-10.1086_210276 Book Reviews      1999 ##  6 journal-article-10.1086_210278 Book Reviews      1999 ##  7 journal-article-10.1086_210279 Book Reviews      1999 ##  8 journal-article-10.1086_210280 Book Reviews      1999 ##  9 journal-article-10.1086_210281 Book Reviews      1999 ## 10 journal-article-10.1086_210283 Book Reviews      1999 ## # … with 180 more rows research_articles <- imported_metadata %>%    filter(article_type == \"research-article\") %>%    filter(!str_detect(article_title, \"^Book Review\"))"},{"path":"https://docs.ropensci.org/jstor/articles/analysing-n-grams.html","id":"the-moving-wall---filtering-articles-by-time","dir":"Articles","previous_headings":"Exploration","what":"The moving wall - filtering articles by time","title":"Analysing n-grams with `jstor` for R","text":"Since JSTOR moving wall, take look number articles per year dataset. plot chunk publications-per-year graph can see increase research articles 2010, number articles first tapers , drops sharply. reason exclude articles least 2015 onward, since sample might get quite biased toward specific journals.","code":"research_articles %>%    ggplot(aes(pub_year)) +   geom_bar() without_wall <- research_articles %>%    filter(pub_year < 2015)"},{"path":"https://docs.ropensci.org/jstor/articles/analysing-n-grams.html","id":"flagship-journals---filtering-articles-by-journal","dir":"Articles","previous_headings":"Exploration","what":"Flagship journals - filtering articles by journal","title":"Analysing n-grams with `jstor` for R","text":"Since amount articles still rather large demonstration, select journals. , look articles two leading journals within discipline, “American Journal Sociology” “American Sociological Review”. Since cleaned identifiers journals jst_augment earlier, can select two flagship-journals easily.","code":"flagship_journals <- without_wall %>%   filter(journal_id %in% c(\"amerjsoci\", \"amersocirevi\"))"},{"path":"https://docs.ropensci.org/jstor/articles/analysing-n-grams.html","id":"importing-bigrams","dir":"Articles","previous_headings":"","what":"Importing bigrams","title":"Analysing n-grams with `jstor` for R","text":"Disclaimer: Much following analysis inspired book “Text Mining R” Julia Silge David Robinson: https://www.tidytextmining.com demonstration look bigrams find common pairs words. now, dealing metadata, therefore need way link reduced dataset bigram files DfR. directory structure deliveries DfR looks something like : structure can see, file name can serve identifier match articles n-grams, since similar metadata n-grams. make importing subset ngrams convenient, can use jst_subset_ngrams. function returns list “zip-locations”, jst_get_ngram can read. 872 articles two flagship journals now 6,729,813 bigrams. bigrams calculated JSTOR article independently. order reduce sample common bigrams, two choices: either include terms occur within article given amount times, include terms occur within articles given amount times. including terms occur 5 times article, can drastically reduce number terms. However, might miss important ones: might terms occur repeatedly within articles, present . demonstration purposes bit restrictive include terms, occur least three times per article.","code":"receipt-id-123456-part-001   -- metadata     -- journal_article_foo.xml        .        .        .   -- ngram2     -- journal_article_foo.txt        .        .        . receipt-id-123456-part-002   -- metadata     -- journal_article_bar.xml        .        .        .   -- ngram2     -- journal_article_bar.txt        .        .        . ngram_selection <- jst_subset_ngrams(c(\"part1.zip\", \"part2.zip\"), \"ngram2\",                                     flagship_journals) ## Error in utils::unzip(zip_archive, list = TRUE): zip Datei 'part2.zip' kann nicht geöffnet werden head(ngram_selection, 2) ## Error in head(ngram_selection, 2): Objekt 'ngram_selection' nicht gefunden ngram_selection <- jst_subset_ngrams(c(\"part1.zip\", \"part2.zip\"), \"ngram2\",                                     flagship_journals)  imported_bigrams <- ngram_selection %>%    furrr::future_map_dfr(jst_get_ngram) top_bigrams <- imported_bigrams %>%   filter(n >= 3)"},{"path":"https://docs.ropensci.org/jstor/articles/analysing-n-grams.html","id":"cleaning-up-bigrams","dir":"Articles","previous_headings":"Importing bigrams","what":"Cleaning up bigrams","title":"Analysing n-grams with `jstor` for R","text":"constructing n-grams, DfR uses stop-word list, quite limited 2. like restrict terms bit , use stopwords tidytext: removing stopwords need consider fact, bigrams created article . order analyse together, need count terms articles combination. first terms can see, still many terms interesting analysis. terms “american” “sociological” simply part title journal selected (American Sociological Review). clean terms , can employ different approaches. One simply filter terms wish exclude: look another approach plotting bigrams.","code":"library(tidytext) bigrams_separated <- top_bigrams %>%   separate(bigrams, c(\"word1\", \"word2\"), sep = \" \")  bigrams_filtered <- bigrams_separated %>%   filter(!word1 %in% stop_words$word) %>%   filter(!word2 %in% stop_words$word) bigram_counts <- bigrams_filtered %>%   group_by(word1, word2) %>%   summarise(n = sum(n)) %>%   arrange(desc(n))  bigram_counts ## # A tibble: 106,706 x 3 ## # Groups:   word1 [18,152] ##    word1        word2            n ##    <chr>        <chr>        <int> ##  1 american     sociological  9593 ##  2 sociological review        9198 ##  3 university   press         4603 ##  4 labor        market        3555 ##  5 american     journal       3273 ##  6 9            7             3270 ##  7 7            6             3260 ##  8 10           9             3230 ##  9 amsmath      amsxtra       3192 ## 10 begin        document      3192 ## # … with 106,696 more rows bigram_counts_clean <- bigram_counts %>%   unite(bigram, word1, word2, sep = \" \") %>%   filter(!bigram %in% c(\"american sociological\", \"sociological review\",                         \"university press\", \"american journal\",                         \"journal sociology\")) %>%   separate(bigram, c(\"word1\", \"word2\"))"},{"path":"https://docs.ropensci.org/jstor/articles/analysing-n-grams.html","id":"visualize-relationships","dir":"Articles","previous_headings":"","what":"Visualize relationships","title":"Analysing n-grams with `jstor` for R","text":"analyzing bigrams, might want look relationships common terms. can leverage power igraph ggraph. First, keep common terms convert data.frame igraph-object. 3 plotting, use simple plotting function, adapted https://www.tidytextmining.com/ngrams.html#visualizing--network--bigrams--ggraph. obvious group nodes relevant topic inequality. come LaTeX documents somehow made way original dataset. However, since common terms, quite easy remove. can look nodes/vertices graph V(bigram_graph). first node, “labor”, relevant us, nodes 2 least 40 clearly irrelevant. can remove simple subtraction: Another apparent group combination “table” “figure” digits. evidently comes tables figures papers might suggest, articles sample quite frequently employ quantitative methods, figures tables common. analysis hand however, might remove , along irrelevant terms. cleaning bit, can take fresh look bigrams. plot chunk second-bigram-graph figure still far perfect (“eco” -> “nomic” clearly one term), can begin analyse network. frequent bigrams now “labor market”, “labor force”, “income inequality”, surprising given individuals capitalist societies need supply work exchange income. reason, labor market stratification prime subject sociological inquiry inequality. key dimensions sociological analysis apparent graph: gender, race/ethnicity, occupational socioeconomic status. find many terms associated term “social” seems quite likely given discipline’s subject. least two surprising results pointed . First, evident terms “ethnic” “racial” connected. form typical term like “social capital”, “middle class” similar, considered dichotomy like “black” “white” often included tables regressions. theoretical point view, slightly different meanings frequently used synonyms. Second, group nodes around term “university”: university -> chicago, university -> california, harvard -> university, etc. least two explanations seem plausible: either, many books cited way associated universities (“University Chicago Press” largest university press United States), many researchers publish two flagship-journals selected affiliated four universities: Harvard, Chicago, Cambridge California. least partly prominence university -> chicago -> press might due fact, publisher American Journal Sociology, therefore included article journal. Altogether, findings surprising well-educated sociologists. Almost bigrams graph common concepts terms within discipline. importantly, often concepts terms comprise two single words, “social science”, “social structure”, “social networks”. Two approaches might useful examine, concepts used: Analyzing trigrams. case, many concepts show combination interesting terms, analyze combinations three words. Another approach first analyze data bigrams , determining core concepts field. second step, one search core concepts raw data files extract adjacent terms. possible default deliveries DfR however, since full-text content available dedicated agreement.","code":"library(igraph) library(ggraph) bigram_graph <- bigram_counts_clean %>%   filter(n > 500) %>%   graph_from_data_frame()  bigram_graph ## IGRAPH 755e99b DN-- 170 161 --  ## + attr: name (v/c), n (e/n) ## + edges from 755e99b (vertex names): ##  [1] labor                 ->market   9                     ->7        ##  [3] 7                     ->6        10                    ->9        ##  [5] amsmath               ->amsxtra  begin                 ->document ##  [7] declaremathsizes      ->10       declaretextfontcommand->textcyr  ##  [9] documentclass         ->aastex   encodingdefault       ->ot2      ## [11] newcommand            ->cyr      ot1                   ->fontenc  ## [13] ot2                   ->ot1      pagestyle             ->empty    ## [15] portland              ->xspace   ## + ... omitted several edges plot_bigrams <- function(igraph_df, seed = 2016) {   set.seed(seed)      a <- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))    ggraph(igraph_df, layout = \"fr\") +     geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,                    arrow = a, end_cap = circle(.07, 'inches')) +     geom_node_point(color = \"lightblue\", size = 4) +     geom_node_text(aes(label = name), repel = T) +     theme_graph() } plot_bigrams(bigram_graph) V(bigram_graph) ## + 170/170 vertices, named, from 755e99b: ##   [1] labor                  9                      7                      ##   [4] 10                     amsmath                begin                  ##   [7] declaremathsizes       declaretextfontcommand documentclass          ##  [10] encodingdefault        newcommand             ot1                    ##  [13] ot2                    pagestyle              portland               ##  [16] renewcommand           rmdefault              sfdefault              ##  [19] textcyr                usepackage             6                      ##  [22] aastex                 amsbsy                 amsfonts               ##  [25] amssymb                amsxtra                bm                     ##  [28] cyr                    document               empty                  ## + ... omitted several vertices bigram_graph_clean <- bigram_graph - 2:40 bigram_graph_clean ## IGRAPH 3272619 DN-- 131 105 --  ## + attr: name (v/c), n (e/n) ## + edges from 3272619 (vertex names): ##  [1] labor     ->market     labor     ->force      0         ->0          ##  [4] table     ->2          1         ->1          income    ->inequality ##  [7] black     ->white      table     ->1          table     ->3          ## [10] social    ->capital    model     ->1          model     ->2          ## [13] african   ->american   0         ->1          human     ->capital    ## [16] african   ->americans  table     ->4          1         ->2          ## [19] model     ->3          racial    ->ethnic     individual->level      ## [22] civil     ->rights     cross     ->national   ## + ... omitted several edges bigram_graph_clean <- bigram_graph_clean - c(\"table\", \"model\",                                              as.character(0:5),                                              \"xd\", \"rh\", \"landscape\", \"00\",                                              \"figure\", \"review\", \"79\",                                              \"http\", \"www\", \"000\", \"01\") plot_bigrams(bigram_graph_clean, 234)"},{"path":"https://docs.ropensci.org/jstor/articles/analysing-n-grams.html","id":"comparison-over-time","dir":"Articles","previous_headings":"","what":"Comparison over time","title":"Analysing n-grams with `jstor` for R","text":"Besides looking overall relationship bigrams, interested development time specific terms. , want look often “labor market” “income inequality” appear year year. , need join bigrams metadata. , need sum counts, time grouped year: now keep two terms interest plot simple chart. plot chunk bigrams--time instance, plot reveal trends time-frequency terms fluctuating lot staying similar level. Single spikes term frequency specific years (example income inequality 2011) stem special issues explicitly concerned income inequality, although quick glance corresponding issues invalidates hypothesis.","code":"time_bigrams <- top_bigrams %>%    left_join(flagship_journals, by = \"file_name\") %>%    select(bigrams, n, pub_year)  head(time_bigrams) ##             bigrams  n pub_year ## 1    private sector 92     1998 ## 2 market transition 68     1998 ## 3               3 t 38     1998 ## 4              1 00 37     1998 ## 5 journal sociology 34     1998 ## 6      state sector 34     1998 time_bigrams <- time_bigrams %>%   group_by(bigrams, pub_year) %>%   summarise(n = sum(n)) %>%   arrange(desc(n))  time_bigrams ## # A tibble: 248,725 x 3 ## # Groups:   bigrams [157,266] ##    bigrams               pub_year     n ##    <chr>                    <int> <int> ##  1 0 0                       2004  1071 ##  2 et al                     2014   916 ##  3 women s                   2006   885 ##  4 american sociological     2014   860 ##  5 sociological review       2014   814 ##  6 u s                       2014   793 ##  7 et al                     2011   792 ##  8 et al                     2013   748 ##  9 et al                     2010   691 ## 10 et al                     2012   687 ## # … with 248,715 more rows # filter the terms of interest time_comparison <- time_bigrams %>%    filter(bigrams == \"labor market\" | bigrams == \"income inequality\")  ggplot(time_comparison, aes(pub_year, n, colour = bigrams)) +   geom_point() +   geom_line() +   scale_x_continuous(breaks = scales::pretty_breaks(7))"},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"intro","dir":"Articles","previous_headings":"","what":"Intro","title":"Automating File Import","text":"find_* functions jstor work single file. Data DfR however contains many single files, 25,000 using self-service functions, several hundreds thousands files requesting large dataset via agreement. Currently jstor offers two implementations import many files: jst_import_zip() jst_import(). first one lets import data directly zip archives, second works file paths, need unzip archive first. first introduce jst_import_zip() discuss approach jst_import() afterwards.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"importing-data-directly-from-zip-archives","dir":"Articles","previous_headings":"","what":"Importing data directly from zip-archives","title":"Automating File Import","text":"Unpacking working many files directly unpractical least three reasons: unzip archive, single files occupy lot disk space single archive. can import files, need list via list.files system(\"find...\") UNIX. Depending size data, can take time. might different types data sample (journal articles, book chapters, etc.). need manage matching paths appropriate functions, extra work. Importing directly zip archive simplifies tasks single function: jst_import_zip(). following demonstration, use small sample archive comes package. first step, take look archive content. made easy jst_preview_zip(): can see simple archive three metadata files one ngram file. can use jst_import_zip(), first need think , actually want import: data, just parts? kind data want extract articles, books pamphlets? can specify via jst_define_import(): case, want import data articles (standard metadata plus information authors), general data books unigrams (ngram1). specification can used jst_import_zip(): can take look files within tmp list.files(): can see, jst_import_zip() automatically creates file names based string supplied out_file delineate different types output. want re-import data analysis, can either use functions like readr::read_csv(), small helper function package determines sets column types correctly: side note ngrams: larger archives, importing ngrams can take long time. thus advisable import ngrams articles want analyse, .e. likely subset initial request. function jst_subset_ngrams() helps (see also section importing bigrams case study).","code":"jst_preview_zip(jst_example(\"pseudo_dfr.zip\")) %>% knitr::kable() import_spec <- jst_define_import(   article = c(jst_get_article, jst_get_authors),   book = jst_get_book,   ngram1 = jst_get_ngram ) # set up a temporary folder for output files tmp <- tempdir()  # extract the content and write output to disk jst_import_zip(jst_example(\"pseudo_dfr.zip\"),                import_spec = import_spec,                out_file = \"my_test\",                out_path = tmp) #> Processing files for book_chapter with functions jst_get_book #> Processing files for journal_article with functions jst_get_article, jst_get_authors #> Processing files for ngram1 with functions jst_get_ngram list.files(tmp, pattern = \"csv\") #> [1] \"my_test_book_chapter_jst_get_book-1.csv\"       #> [2] \"my_test_journal_article_jst_get_article-1.csv\" #> [3] \"my_test_journal_article_jst_get_authors-1.csv\" #> [4] \"my_test_ngram1_jst_get_ngram-1.csv\" jst_re_import(   file.path(tmp, \"my_test_journal_article_jst_get_article-1.csv\") ) %>%    knitr::kable()"},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"parallel-processing","dir":"Articles","previous_headings":"Importing data directly from zip-archives","what":"Parallel processing","title":"Automating File Import","text":"Since process might take larger archives (files unpacked, read parsed), might benefit executing function parallel. jst_import_zip() jst_import() use furrr core, therefore adding parallelism easy. Just add following lines beginning script, import use available cores: can find futures reading package vignette:","code":"library(future)  plan(multisession) vignette(\"future-1-overview\", package = \"future\")"},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"working-with-file-paths","dir":"Articles","previous_headings":"","what":"Working with file paths","title":"Automating File Import","text":"approach importing directly zip archives convenient, cases might want control data imported. example, run problems output functions provided package looks corrupted, want look original files. Alongside , unzip archive work files directly, demonstrate following sections.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"unzip-containers","dir":"Articles","previous_headings":"Working with file paths","what":"Unzip containers","title":"Automating File Import","text":"simple purposes might sensible unzip temporary directory (temp() unzip()) research simply extracted files external SSD, since ) lacked disk space, b) needed read fast, c) wanted able look specific files debugging.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"list-files","dir":"Articles","previous_headings":"Working with file paths","what":"List files","title":"Automating File Import","text":"many ways generate list files: list.files() using system() conjunction find unix-like systems common options. demonstration purposes use files contained jstor can accessed via system.file:","code":"example_dir <- system.file(\"extdata\", package = \"jstor\")"},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"list-files-1","dir":"Articles","previous_headings":"Working with file paths","what":"list.files","title":"Automating File Import","text":"","code":"file_names_listed <- list.files(path = example_dir, full.names = TRUE,                                 pattern = \"*.xml\") file_names_listed #> [1] \"/usr/local/lib/R/site-library/jstor/extdata/article_with_footnotes.xml\"  #> [2] \"/usr/local/lib/R/site-library/jstor/extdata/article_with_references.xml\" #> [3] \"/usr/local/lib/R/site-library/jstor/extdata/book.xml\"                    #> [4] \"/usr/local/lib/R/site-library/jstor/extdata/parsed_references.xml\""},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"system-and-find","dir":"Articles","previous_headings":"Working with file paths > list.files","what":"system and find","title":"Automating File Import","text":"case two approaches give result. main difference seems though, list.files sorts output, whereas find . large amount files (200,000) makes list.files slower, smaller datasets difference shouldn’t make impact.","code":"file_names <- system(paste0(\"cd \", example_dir, \"; find . -name '*.xml' -type f\"), intern = TRUE) library(stringr)  file_names_system <- file_names %>%   str_replace(\"^\\\\.\\\\/\", \"\") %>%   str_c(example_dir, \"/\", .)  file_names_system #> [1] \"/Library/Frameworks/R.framework/Versions/3.4/Resources/library/jstor/extdata/sample_with_footnotes.xml\"  #> [2] \"/Library/Frameworks/R.framework/Versions/3.4/Resources/library/jstor/extdata/sample_book.xml\" #> [3] \"/Library/Frameworks/R.framework/Versions/3.4/Resources/library/jstor/extdata/sample_with_references.xml\""},{"path":"https://docs.ropensci.org/jstor/articles/automating-file-import.html","id":"batch-import","dir":"Articles","previous_headings":"Working with file paths","what":"Batch import","title":"Automating File Import","text":"file list generated, can apply jst_get_*-functions list. good simple way small moderate amounts files use purrr::map_df(): works well 1) errors 2) moderate size files. larger numbers files, jst_import() can streamline process . function works similar jst_import_zip(), main difference needs file paths input can handle one type output. result written disk, can seen :","code":"# only work with journal articles article_paths <- file_names_listed %>%    keep(str_detect, \"with\")  article_paths %>%    map_df(jst_get_article) %>%    knitr::kable() jst_import(article_paths, out_file = \"my_second_test\", .f = jst_get_article,             out_path = tmp) #> Starting to import 2 file(s). #> Finished importing 2 file(s) in 0.03 secs. list.files(tmp, pattern = \"csv\") #> [1] \"my_second_test-1.csv\"                          #> [2] \"my_test_book_chapter_jst_get_book-1.csv\"       #> [3] \"my_test_journal_article_jst_get_article-1.csv\" #> [4] \"my_test_journal_article_jst_get_authors-1.csv\" #> [5] \"my_test_ngram1_jst_get_ngram-1.csv\""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"collecting-all-the-quirks","dir":"Articles","previous_headings":"","what":"Collecting all the quirks","title":"Known Quirks of JSTOR/DfR Data","text":"Data JSTOR/DfR unlike data encounter text analysis. First foremost, data articles books come wide variety journals publishers. level detail certain formats vary . jstor tries deal situation two strategies: try recognise format read data accordingly possible, read data “raw” possible, .e. without conversions example first case references. Four different ways references can specified known time, imported specific ways deal variation. might however formats, lead informative error trying import via jst_get_references(). example latter case page numbers. time, entries page numbers simply 42, 61. expected, parsed integers. Sometimes, characters present, like M77. pose problem either, simply extract digits via regex parse character. Unfortunately, sometimes page specified like : v75i2p84. Extracting digits result 75284, wrong long shot. Since might ways specifying pages, jstor attempt parse pages integers importing. However, offers set convenience functions deal common cases (see jst_augment() ). many problems peculiarities like . vignette tries list many possible, offer solutions dealing . Unfortunately neither time interest wade data get DfR order find possible quirks. following list thus inevitably incomplete. encounter new quirks/peculiarities, greatly appreciated sent email, opened issue GitHub. include findings future version vignette, vignette can starting point everybody conducts text analysis data JSTOR/DfR.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"data-augmentation","dir":"Articles","previous_headings":"","what":"Data augmentation","title":"Known Quirks of JSTOR/DfR Data","text":"importing data via jst_get_article(), least two tasks might typically want undertake: Merge different identifiers journals one, can filter journals. Convert pages character integers calculate total number pages per article. four functions help streamline process: jst_clean_page() attempts turn character vector pages integer vector. jst_add_total_pages() adds column total number pages per article. jst_unify_journal_id() merges different identifiers journals one. jst_augment() wraps functions convenience.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"known-quirks","dir":"Articles","previous_headings":"","what":"Known quirks","title":"Known Quirks of JSTOR/DfR Data","text":"following sections, known issues data DfR described greater detail.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"page-numbers","dir":"Articles","previous_headings":"Known quirks","what":"Page numbers","title":"Known Quirks of JSTOR/DfR Data","text":"Page numbers mess. Besides issues mentioned , page numbers might sometimes specified “pp. 1234-83” article American Journal Sociology. course, results first_page = 1234 last_page = 83, computed number total pages jst_get_total_pages() negative. currently general solution issue.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"calculating-total-pages","dir":"Articles","previous_headings":"Known quirks > Page numbers","what":"Calculating total pages","title":"Known Quirks of JSTOR/DfR Data","text":"outlined , page numbers come different forms. Besides problem, actually another issue. Imagine like quantify lengths articles. Obviously need information first last page articles. Furthermore, pages need parsed properly: run troubles calculate page numbers like 75284 - 42 + 1, case number parsed badly. jst_clean_page() tries properly, based known possibilities: “2” -> 2 “A2” -> 2 “v75i2p84” -> 84 Parsing correctly unfortunately enough. Things like “Errata” might come haunt . example might article first_page = 42 last_page = 362, leave puzzled can true1. simple explanation: article might start page 42, end page 65, furthermore erratum page 362. Technically, last_page = 362 true , cause problems calculating total number pages. Quite often, information another column resolve : page_range, case look like 42 - 65, 362. small helper deal situations jst_get_total_pages(). works page ranges, also first last pages: actually identical using jst_add_total_pages():","code":"library(jstor) library(dplyr)  input <- tibble::tribble(   ~first_page,   ~last_page,    ~page_range,   NA_real_,      NA_real_,      NA_character_,    1,             10,            \"1 - 10\",   1,             10,            NA_character_,   1,             NA_real_,      NA_character_,    1,             NA_real_,      \"1-10\",   NA_real_,       NA_real_,      \"1, 5-10\",   NA_real_,       NA_real_,      \"1-4, 5-10\",   NA_real_,       NA_real_,      \"1-4, C5-C10\" )  input %>%    mutate(n_pages = jst_get_total_pages(first_page, last_page, page_range)) #> # A tibble: 8 × 4 #>   first_page last_page page_range  n_pages #>        <dbl>     <dbl> <chr>         <dbl> #> 1         NA        NA NA               NA #> 2          1        10 1 - 10           10 #> 3          1        10 NA               10 #> 4          1        NA NA               NA #> 5          1        NA 1-10             10 #> 6         NA        NA 1, 5-10           7 #> 7         NA        NA 1-4, 5-10        10 #> 8         NA        NA 1-4, C5-C10      10 input %>%    jst_add_total_pages() #> # A tibble: 8 × 4 #>   first_page last_page page_range  n_pages #>        <dbl>     <dbl> <chr>         <dbl> #> 1         NA        NA NA               NA #> 2          1        10 1 - 10           10 #> 3          1        10 NA               10 #> 4          1        NA NA               NA #> 5          1        NA 1-10             10 #> 6         NA        NA 1, 5-10           7 #> 7         NA        NA 1-4, 5-10        10 #> 8         NA        NA 1-4, C5-C10      10"},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"journal-identifiers","dir":"Articles","previous_headings":"Known quirks","what":"Journal identifiers","title":"Known Quirks of JSTOR/DfR Data","text":"Identifiers journal usually appear three columns: journal_doi journal_jcode journal_pub_id samples, seems information journal_pub_id often missing, journal_doi. important identifier thus journal_jcode. cases journal_jcode journal_pub_id present, least samples, format journal_jcode different. consistency, jst_unify_journal_id() thus takes content journal_pub_id present, journal_jcode otherwise. algorithm, possible reliably match general information respective journals, available jst_get_journal_overview():","code":"sample_article <- jst_get_article(jst_example(\"article_with_references.xml\"))   knitr::kable(sample_article) sample_article %>%    jst_unify_journal_id() %>%    left_join(jst_get_journal_overview()) %>%    tidyr::gather(variable, value) %>%    knitr::kable() #> Joining with `by = join_by(journal_id)`"},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"duplicated-ngrams","dir":"Articles","previous_headings":"Known quirks","what":"Duplicated ngrams","title":"Known Quirks of JSTOR/DfR Data","text":"AJS, ngrams book reviews calculated per issue. numerous reviews per issue, identical file ngrams, containing ngrams book reviews issue. possible strategy dealing either use ngrams, since calculated reviews issue, irrespective whether actually reviews given issue sample . Alternatively, one group issues, take one set ngrams per issue.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"language-codes","dir":"Articles","previous_headings":"Known quirks","what":"Language codes","title":"Known Quirks of JSTOR/DfR Data","text":"Information langues consistent. sample article, language eng. cases might en. thus advisable take quick look different variants via distinct(meta_data, language) count(meta_data, language).","code":"sample_article %>%    pull(language) #> [1] \"eng\""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"incorrectodd-references","dir":"Articles","previous_headings":"Known quirks","what":"Incorrect/odd references","title":"Known Quirks of JSTOR/DfR Data","text":"analysing data references footnotes, encounter many inconsistencies errors. due errors DfR, stem simply fact, humans make mistakes creating manuscripts, errors references caught printing.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"problems-with-non-english-characters","dir":"Articles","previous_headings":"Known quirks > Incorrect/odd references","what":"Problems with non-english characters","title":"Known Quirks of JSTOR/DfR Data","text":"common problem names non-english characters like german umlauts (Ferdinand Tönnies) nordic names (Gøsta Esping-Andersen). appear many different variations: Tonnies, Tönnies, Gosta, Gösta, etc.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"ocr-issues","dir":"Articles","previous_headings":"Known quirks > Incorrect/odd references","what":"OCR-Issues","title":"Known Quirks of JSTOR/DfR Data","text":"older articles, might encounter issues stem digitising text OCR-software. common problem distinguishing l, like phrase “love”. Depending names appear data, might lead inconsistencies.","code":""},{"path":"https://docs.ropensci.org/jstor/articles/known-quirks.html","id":"errors-by-article-authors","dir":"Articles","previous_headings":"Known quirks > Incorrect/odd references","what":"Errors by article authors","title":"Known Quirks of JSTOR/DfR Data","text":"many examples authors make mistakes summary statistics end skewed. article “Ethics Education Workplace” cites items multiple times, possibly artifact. advantage using JSTOR/DfR data , can inspect sources check, specific pattern see artifact genuine.","code":""},{"path":"https://docs.ropensci.org/jstor/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Thomas Klebel. Author, maintainer.","code":""},{"path":"https://docs.ropensci.org/jstor/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Klebel (2018). jstor: Import Analyse Data Scientific Texts. Journal Open Source Software, 3(28), 883, https://doi.org/10.21105/joss.00883","code":"@Article{,   author = {Thomas Klebel},   title = {jstor: Import and Analyse Data from Scientific Texts},   journal = {Journal of Open Source Software},   volume = {3},   issue = {28},   page = {883},   year = {2018},   doi = {10.21105/joss.00883},   url = {https://ropensci.github.io/jstor/}, }"},{"path":"https://docs.ropensci.org/jstor/index.html","id":"jstor-import-and-analyse-data-from-scientific-articles","dir":"","previous_headings":"","what":"Read Data from JSTOR/DfR","title":"Read Data from JSTOR/DfR","text":"Author: Thomas Klebel License: GPL v3.0  tool Data Research (DfR) JSTOR valuable source citation analysis text mining. jstor provides functions suggests workflows importing datasets DfR. developed deal large datasets require agreement, can used smaller ones well. Note: 2021, JSTOR moved changed way provide data new platform called Constellate. package jstor adapted change, might therefore used legacy data optained old DfR platform. important set functions group jst_get_* functions: jst_get_article jst_get_authors jst_get_references jst_get_footnotes jst_get_book jst_get_chapters jst_get_full_text jst_get_ngram functions concerned meta data (therefore excluding jst_get_full_text jst_get_ngram) operate along lines: file read xml2::read_xml(). Content file extracted via XPATH CSS-expressions. resulting data returned tibble.","code":""},{"path":"https://docs.ropensci.org/jstor/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Read Data from JSTOR/DfR","text":"install package use: can install development version GitHub :","code":"install.packages(\"jstor\") # install.packages(\"remotes\") remotes::install_github(\"ropensci/jstor\")"},{"path":"https://docs.ropensci.org/jstor/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Read Data from JSTOR/DfR","text":"order use jstor, first need load : basic usage simple: supply one jst_get_*-functions path return tibble extracted information. explanations, especially use jstor’s functions importing many files, can found vignettes.","code":"library(jstor) library(magrittr) jst_get_article(jst_example(\"article_with_references.xml\")) %>% knitr::kable() jst_get_authors(jst_example(\"article_with_references.xml\")) %>% knitr::kable()"},{"path":"https://docs.ropensci.org/jstor/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting started","title":"Read Data from JSTOR/DfR","text":"order use jstor, need data DfR. main page can create dataset searching terms restricting search regarding time, subject content type. created account, can download selection. Alternatively, can download sample datasets documents 1923 US, 1870 countries.","code":""},{"path":"https://docs.ropensci.org/jstor/index.html","id":"supported-elements","dir":"","previous_headings":"","what":"Supported Elements","title":"Read Data from JSTOR/DfR","text":"technical specifications, DfR lists fields reliably present articles books. following table gives overview, elements supported jstor.","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://docs.ropensci.org/jstor/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of conduct","title":"Read Data from JSTOR/DfR","text":"Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"https://docs.ropensci.org/jstor/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Read Data from JSTOR/DfR","text":"cite jstor, please refer citation(package =     \"jstor\"):","code":"Klebel (2018). jstor: Import and Analyse Data from Scientific Texts. Journal of  Open Source Software, 3(28), 883, https://doi.org/10.21105/joss.00883"},{"path":"https://docs.ropensci.org/jstor/index.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"Read Data from JSTOR/DfR","text":"Work jstor benefited financial support project “Academic Super-Elites Sociology Economics” Austrian Science Fund (FWF), project number “P 29211 Einzelprojekte”. internal functions regarding file paths example files adapted package readr.","code":""},{"path":"https://docs.ropensci.org/jstor/paper.html","id":null,"dir":"","previous_headings":"","what":"Summary","title":"Summary","text":"interest text data seen sharp increase past years, mostly due advent methods automated text analysis. time, researches within field scientometrics analysed citations aspects scholarly literature great sophistication. archival content JSTOR offers rich diverse set primary sources like research articles book chapters approaches. Data Research (DfR) JSTOR gives researchers, regardless whether access JSTOR , opportunity analyse metadata, n-grams , upon special request, full-text materials available articles books JSTOR. package jstor [@jstor] helps analysing datasets enabling researchers easily import metadata R [@r_core], task, integrated solution exists date. metadata DfR can either analysed used conjunction n-grams full-text data. Commonly, metadata DfR include information articles’ authors, title, journal, date publishing, quite frequently footnotes references. information can interest specific research questions. analysis n-grams full-texts, metadata imported jstor allow researchers filter articles based specific journals, dates publication, authors, keywords titles aspects. jstor provides functions three main tasks within research process: Importing different parts metadata, either XML-files directly .zip-archive provided DfR. Importing n-gram full-text files. Performing common tasks cleaning metadata like unifying journal id cleaning page numbers. Full documentation jstor, including comprehensive case study analysing n-grams DfR, available https://ropensci.github.io/jstor/. package can obtained CRAN (https://CRAN.R-project.org/package=jstor) GitHub (https://github.com/ropensci/jstor). Archived versions releases available Zenodo (https://doi.org/10.5281/zenodo.1169861).","code":""},{"path":"https://docs.ropensci.org/jstor/paper.html","id":"acknowledgements","dir":"","previous_headings":"","what":"Acknowledgements","title":"Summary","text":"indebted Jason Becker Elin Waring helpful requests package review. Benjamin Klebel, Antonia Schirgi Matthias Duller provided helpful comments requests clarifications writing case study. Work jstor benefited financial support project “Academic Super-Elites Sociology Economics” Austrian Science Fund (FWF), project number “P 29211 Einzelprojekte”. jstor currently used project team analyse academic elites sociology economics.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/jstor/reference/jst_add_total_pages.html","id":null,"dir":"Reference","previous_headings":"","what":"Add total count of pages — jst_add_total_pages","title":"Add total count of pages — jst_add_total_pages","text":"function adds column total count pages. calls jst_get_total_pages() main work.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_add_total_pages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add total count of pages — jst_add_total_pages","text":"","code":"jst_add_total_pages(meta_data, quietly = FALSE)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_add_total_pages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add total count of pages — jst_add_total_pages","text":"meta_data Data processed via jst_get_article(). quietly warnings converting page ranges suppressed?","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_add_total_pages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Add total count of pages — jst_add_total_pages","text":"tibble, provided meta_data, additional column total number pages.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/jstor/reference/jst_augment.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean data from DfR — jst_augment","title":"Clean data from DfR — jst_augment","text":"function takes data jst_get_article() applies helper functions cleaning data.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_augment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean data from DfR — jst_augment","text":"","code":"jst_augment(meta_data, quietly = FALSE)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_augment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean data from DfR — jst_augment","text":"meta_data Data processed via jst_get_article(). quietly warnings converting page ranges suppressed?","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_augment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean data from DfR — jst_augment","text":"cleaned tibble.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_augment.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Clean data from DfR — jst_augment","text":"Data DfR inherently messy. many examples see vignette(\"known-quirks\", package = \"jstor\"). jst_augment() convenience function tries deal common tasks clean data. journal articles, calls jst_clean_page() convert first last page, jst_unify_journal_id() jst_add_total_pages().","code":""},{"path":[]},{"path":"https://docs.ropensci.org/jstor/reference/jst_clean_page.html","id":null,"dir":"Reference","previous_headings":"","what":"Clean a character vector of pages — jst_clean_page","title":"Clean a character vector of pages — jst_clean_page","text":"function tries convert character vectors integers. function called page ranges.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_clean_page.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clean a character vector of pages — jst_clean_page","text":"","code":"jst_clean_page(page)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_clean_page.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clean a character vector of pages — jst_clean_page","text":"page character vector pages.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_clean_page.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clean a character vector of pages — jst_clean_page","text":"integer vector, cleaned converted input vector.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_clean_page.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clean a character vector of pages — jst_clean_page","text":"","code":"jst_clean_page(\"2\") #> [1] 2  # anything that is not a digit gets removed jst_clean_page(\"A2-\") #> [1] 2  # a weird format from the American Journal of Sociology is convered correctly jst_clean_page(\"AJSv104p126\") #> [1] 126 # this is done by searching for \"p\", and if it is found, extracting the # content after \"p\"."},{"path":"https://docs.ropensci.org/jstor/reference/jst_combine_outputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine outputs from converted files — jst_combine_outputs","title":"Combine outputs from converted files — jst_combine_outputs","text":"jst_combine_outputs() helps manage multitude files might receive running jst_import() jst_import_zip() one batch.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_combine_outputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine outputs from converted files — jst_combine_outputs","text":"","code":"jst_combine_outputs(   path,   write_to_file = TRUE,   out_path = NULL,   overwrite = FALSE,   clean_up = FALSE,   warn = TRUE )"},{"path":"https://docs.ropensci.org/jstor/reference/jst_combine_outputs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine outputs from converted files — jst_combine_outputs","text":"path path directory, containing .csv-files jst_import() jst_import_zip(), vector files imported. write_to_file combined data written file? out_path directory write combined files. directory supplied write_to_file TRUE, combined files written path. overwrite files overwritten? clean_up want remove original batch files? Use caution. warn warnings raised, file type determined?","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_combine_outputs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine outputs from converted files — jst_combine_outputs","text":"Either writes disk, returns list combined files.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_combine_outputs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Combine outputs from converted files — jst_combine_outputs","text":"Splitting output jst_import() jst_import_zip() might done multiple reasons, end possibly want combine outputs one file/data.frame. function makes assumptions order combine files: Files similar names (except trailing dashes numbers) belong together combined one file. names combined files can determined original files. want combine foo-1.csv foo-2.csv, combined file combined_foo.csv. directory contains files imported via jst_import() jst_import_zip(). directory contains .csv files, supply character vector paths files, want import.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/jstor/reference/jst_combine_outputs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine outputs from converted files — jst_combine_outputs","text":"","code":"# set up a temporary directory tmp <- tempdir()  # find multiple files file_list <- rep(jst_example(\"article_with_references.xml\"), 2)  # convert and write to file jst_import(file_list, \"article\", out_path = tmp, .f = jst_get_article,              n_batches = 2, show_progress = FALSE) #> Starting to import 2 file(s). #> Processing chunk 1/2 #> Processing chunk 2/2 #> Finished importing 2 file(s) in 0.21 secs.               # combine outputs jst_combine_outputs(tmp) #> Re-importing 2 batches. #> Writing combined file `/tmp/RtmpRSxFzb/combined_article.csv` to disk. #> Warning: The `path` argument of `write_csv()` is deprecated as of readr 1.4.0. #> ℹ Please use the `file` argument instead. #> ℹ The deprecated feature was likely used in the jstor package. #>   Please report the issue at <https://github.com/ropensci/jstor/issues>. list.files(tmp, \"csv\") #> [1] \"article-1.csv\"        \"article-2.csv\"        \"combined_article.csv\"  if (FALSE) { # Trying to combine the files again raises an error. jst_combine_outputs(tmp) }  # this doesn't jst_combine_outputs(tmp, overwrite = TRUE) #> Re-importing 2 batches. #> Writing combined file `/tmp/RtmpRSxFzb/combined_article.csv` to disk.  # we can remove the original files too jst_combine_outputs(tmp, overwrite = TRUE, clean_up = TRUE) #> Re-importing 2 batches. #> Writing combined file `/tmp/RtmpRSxFzb/combined_article.csv` to disk. #> Deleting original batches. list.files(tmp, \"csv\") #> [1] \"combined_article.csv\""},{"path":"https://docs.ropensci.org/jstor/reference/jst_define_import.html","id":null,"dir":"Reference","previous_headings":"","what":"Define an import specification — jst_define_import","title":"Define an import specification — jst_define_import","text":"Define parts zip file converted via functions.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_define_import.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define an import specification — jst_define_import","text":"","code":"jst_define_import(...)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_define_import.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define an import specification — jst_define_import","text":"... Named arguments bare function names.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_define_import.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define an import specification — jst_define_import","text":"specification imports necessary jst_import_zip().","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_define_import.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define an import specification — jst_define_import","text":"function accepts following names: article, book, report, pamphlet, ngram1, ngram2, ngram3. corresponding files .zip-archive imported via supplied functions.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_define_import.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define an import specification — jst_define_import","text":"","code":"# articles will be imported via `jst_get_article()` and `jst_get_authors()` jst_define_import(article = c(jst_get_article, jst_get_authors)) #> # A tibble: 1 × 4 #>   meta_type       fun_names evaled_funs  bare_funs                           #>   <chr>           <list>    <named list> <quos>                              #> 1 journal_article <chr [2]> <list [2]>   c(jst_get_article, jst_get_authors)  # define a specification for importing article metadata and unigrams (ngram1) jst_define_import(article = jst_get_article,                   ngram1 = jst_get_ngram) #> # A tibble: 2 × 4 #>   meta_type       fun_names evaled_funs  bare_funs       #>   <chr>           <list>    <named list> <quos>          #> 1 journal_article <chr [1]> <fn>         jst_get_article #> 2 ngram1          <chr [1]> <fn>         jst_get_ngram                                         # import all four types with one function each jst_define_import(article = jst_get_article,                   book = jst_get_book,                   report = jst_get_book,                   pamphlet = jst_get_article) #> # A tibble: 4 × 4 #>   meta_type       fun_names evaled_funs  bare_funs       #>   <chr>           <list>    <named list> <quos>          #> 1 journal_article <chr [1]> <fn>         jst_get_article #> 2 book_chapter    <chr [1]> <fn>         jst_get_book    #> 3 research_report <chr [1]> <fn>         jst_get_book    #> 4 pamphlet        <chr [1]> <fn>         jst_get_article                    # import all four types with multiple functions jst_define_import(article = c(jst_get_article, jst_get_authors, jst_get_references),                   book = c(jst_get_book, jst_get_chapters),                   report = jst_get_book,                   pamphlet = jst_get_article) #> # A tibble: 4 × 4 #>   meta_type       fun_names evaled_funs  bare_funs                               #>   <chr>           <list>    <named list> <quos>                                  #> 1 journal_article <chr [3]> <list [3]>   c(jst_get_article, jst_get_authors, js… #> 2 book_chapter    <chr [2]> <list [2]>   c(jst_get_book, jst_get_chapters)     … #> 3 research_report <chr [1]> <fn>         jst_get_book                          … #> 4 pamphlet        <chr [1]> <fn>         jst_get_article                       …  # if you want to import chapters with authors, you can use an anonymous # function  chapters_w_authors <- function(x) jst_get_chapters(x, authors = TRUE) jst_define_import(book = chapters_w_authors) #> # A tibble: 1 × 4 #>   meta_type    fun_names evaled_funs  bare_funs          #>   <chr>        <list>    <named list> <quos>             #> 1 book_chapter <chr [1]> <fn>         chapters_w_authors   if (FALSE) { # define imports imports <- jst_define_import(article = c(jst_get_article, jst_get_authors))  # convert the files to .csv jst_import_zip(\"my_archive.zip\", out_file = \"my_out_file\",                   import_spec = imports) }"},{"path":"https://docs.ropensci.org/jstor/reference/jst_example.html","id":null,"dir":"Reference","previous_headings":"","what":"Get path to jstor example — jst_example","title":"Get path to jstor example — jst_example","text":"jstor includes several sample files demonstration purposes. helper makes easy access.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_example.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get path to jstor example — jst_example","text":"","code":"jst_example(path = NULL)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_example.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get path to jstor example — jst_example","text":"path Name example file. NULL, example files listed.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_example.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get path to jstor example — jst_example","text":"Either character vector names example files (jst_example() called without supplying argument), character vector indicating path example file.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_example.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get path to jstor example — jst_example","text":"code function adapted package readr.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_example.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get path to jstor example — jst_example","text":"","code":"jst_example() #> [1] \"article_with_footnotes.xml\"  \"article_with_references.xml\" #> [3] \"book.xml\"                    \"full_text.txt\"               #> [5] \"ngram.txt\"                   \"parsed_references.xml\"       #> [7] \"pseudo_dfr.zip\"              jst_example(\"article_with_references.xml\")  #> [1] \"/usr/local/lib/R/site-library/jstor/extdata/article_with_references.xml\""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_article.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract meta information for articles — jst_get_article","title":"Extract meta information for articles — jst_get_article","text":"jst_get_article() extracts meta-data JSTOR-XML files journal articles.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_article.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract meta information for articles — jst_get_article","text":"","code":"jst_get_article(file_path)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_article.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract meta information for articles — jst_get_article","text":"file_path .xml-file journal-article.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_article.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract meta information for articles — jst_get_article","text":"tibble containing extracted meta-data following columns: file_name (chr): file_name original .xml-file. Can used joining parts (authors, references, footnotes, full-texts). journal_doi (chr): registered identifier journal. journal_jcode (chr): identifier journal like \"amerjsoci\" \"American Journal Sociology\". journal_pub_id (chr): Similar journal_jcode. time either one present. journal_title (chr): title journal. article_doi (chr): registered unique identifier article. article_jcode (chr): unique identifier article (DOI). article_pub_id (chr): Infrequent, either part DOI article_jcode. article_type (chr): type article (research-article, book-review, etc.). article_title (chr): title article. volume (chr): volume article published . issue (chr): issue article published . language (chr): language article. pub_day (chr): Publication day, specified. pub_month (chr): Publication month, specified. pub_year (int): Year publication. first_page (int): Page number first page article. last_page (int): Page number last page article. page_range (chr): range pages article. note publication dates: always first entry extracted, correspond oldest date, case one date.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_article.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract meta information for articles — jst_get_article","text":"","code":"jst_get_article(jst_example(\"article_with_references.xml\")) #> # A tibble: 1 × 19 #>   file_name   journal_doi journal_jcode journal_pub_id journal_title article_doi #>   <chr>       <chr>       <chr>         <chr>          <chr>         <chr>       #> 1 article_wi… NA          tranamermicr… NA             Transactions… 10.2307/32… #> # ℹ 13 more variables: article_pub_id <chr>, article_jcode <chr>, #> #   article_type <chr>, article_title <chr>, volume <chr>, issue <chr>, #> #   language <chr>, pub_day <chr>, pub_month <chr>, pub_year <int>, #> #   first_page <chr>, last_page <chr>, page_range <chr>"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_authors.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract author information — jst_get_authors","title":"Extract author information — jst_get_authors","text":"jst_get_authors() extracts information authors JSTOR-XML files.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_authors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract author information — jst_get_authors","text":"","code":"jst_get_authors(file_path)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_authors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract author information — jst_get_authors","text":"file_path .xml-file JSTOR containing meta-data.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_authors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract author information — jst_get_authors","text":"tibble containing extracted authors. empty fields NA_character.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_authors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract author information — jst_get_authors","text":"function returns tibble following six columns: prefix: case prefix name, like \"Dr.\". given_name: author's given name, like \"Albert\". surname: author's surname like \"Einstein\". string_name: cases data name available separate fields, just complete string: \"Albert Einstein\". suffix: suffix name, like \"Jr.\". author_number: authors enumerated order appear data.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_authors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract author information — jst_get_authors","text":"","code":"jst_get_authors(jst_example(\"article_with_references.xml\")) #> # A tibble: 1 × 7 #>   file_name           prefix given_name surname string_name suffix author_number #>   <chr>               <chr>  <chr>      <chr>   <chr>       <chr>          <int> #> 1 article_with_refer… NA     R.         Kudo    NA          NA                 1"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_book.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract meta information for books — jst_get_book","title":"Extract meta information for books — jst_get_book","text":"jst_get_book() extracts meta-data JSTOR-XML files book chapters.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_book.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract meta information for books — jst_get_book","text":"","code":"jst_get_book(file_path)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_book.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract meta information for books — jst_get_book","text":"file_path .xml-file book research report.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_book.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract meta information for books — jst_get_book","text":"tibble containing extracted meta-data following columns: file_name (chr): filename original .xml-file. Can used joining data file. discipline (chr): discipline discipline names used JSTOR. book_id (chr): book id type \"jstor\", registered DOI. book_title (chr): title book. book_subtitle (chr): subtitle book. pub_day (int): Publication day, specified. pub_month (int): Publication month, specified. pub_year (int): Year publication. isbn (chr): One entries book's ISBN. two , separated \"; \". publisher_name (chr): name publisher. publisher_loc (chr): location publisher. n_pages (int): number pages. language (chr): language book. note publication dates: always first entry extracted, correspond oldest date, case one date.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_book.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract meta information for books — jst_get_book","text":"","code":"jst_get_book(jst_example(\"book.xml\")) #> # A tibble: 1 × 13 #>   book_id     file_name discipline    book_title book_subtitle pub_day pub_month #>   <chr>       <chr>     <chr>         <chr>      <chr>           <int>     <int> #> 1 j.ctt24hdz7 book      Political Sc… The 2006 … A Coup to En…      30         4 #> # ℹ 6 more variables: pub_year <int>, isbn <chr>, publisher_name <chr>, #> #   publisher_location <chr>, n_pages <int>, language <chr>"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_chapters.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract information on book chapters — jst_get_chapters","title":"Extract information on book chapters — jst_get_chapters","text":"jst_get_chapters() extracts meta-data JSTOR-XML files book chapters.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_chapters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract information on book chapters — jst_get_chapters","text":"","code":"jst_get_chapters(file_path, authors = FALSE)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_chapters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract information on book chapters — jst_get_chapters","text":"file_path path .xml-file book research report. authors Extracting authors expensive operation makes function ~3 times slower, depending number chapters number authors. Defaults FALSE. Use authors = TRUE import authors .","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_chapters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract information on book chapters — jst_get_chapters","text":"tibble containing extracted meta-data following columns: book_id (chr): book id type \"jstor\", registered DOI. file_name (chr): filename original .xml-file. Can used joining data file. part_id (chr): id part. part_label (chr): label part, specified. part_title (chr): title part. part_subtitle (chr): subtitle part, specified. authors (list): list-column information authors. Can unnested tidyr::unnest(). See examples jst_get_authors(). abstract (chr): abstract part. part_first_page (chr): page part begins.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_chapters.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract information on book chapters — jst_get_chapters","text":"Currently, jst_get_chapters() quite lot slower functions. roughly 10 times slower jst_get_book, depending number chapters extract.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_chapters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract information on book chapters — jst_get_chapters","text":"","code":"# extract parts without authors jst_get_chapters(jst_example(\"book.xml\")) #> # A tibble: 36 × 9 #>    book_id     file_name part_id     part_label part_title part_subtitle authors #>    <chr>       <chr>     <chr>       <chr>      <chr>      <chr>         <chr>   #>  1 j.ctt24hdz7 book      j.ctt24hdz… NA         Front Mat… NA            NA      #>  2 j.ctt24hdz7 book      j.ctt24hdz… NA         Table of … NA            NA      #>  3 j.ctt24hdz7 book      j.ctt24hdz… NA         Acronyms … NA            NA      #>  4 j.ctt24hdz7 book      j.ctt24hdz… NA         Authors’ … NA            NA      #>  5 j.ctt24hdz7 book      j.ctt24hdz… 1.         The enigm… NA            NA      #>  6 j.ctt24hdz7 book      j.ctt24hdz… 2.         ‘Anxiety,… Fiji’s road … NA      #>  7 j.ctt24hdz7 book      j.ctt24hdz… 3.         Fiji’s De… Who, what, w… NA      #>  8 j.ctt24hdz7 book      j.ctt24hdz… 4.         ‘This pro… The aftermat… NA      #>  9 j.ctt24hdz7 book      j.ctt24hdz… 5.         The chang… NA            NA      #> 10 j.ctt24hdz7 book      j.ctt24hdz… 6.         The Fiji … Analyzing th… NA      #> # ℹ 26 more rows #> # ℹ 2 more variables: abstract <chr>, part_first_page <chr>  # import authors too parts <- jst_get_chapters(jst_example(\"book.xml\"), authors = TRUE) parts #> # A tibble: 36 × 9 #>    book_id     file_name part_id    part_label part_title part_subtitle authors  #>    <chr>       <chr>     <chr>      <chr>      <chr>      <chr>         <list>   #>  1 j.ctt24hdz7 book      j.ctt24hd… NA         Front Mat… NA            <tibble> #>  2 j.ctt24hdz7 book      j.ctt24hd… NA         Table of … NA            <tibble> #>  3 j.ctt24hdz7 book      j.ctt24hd… NA         Acronyms … NA            <tibble> #>  4 j.ctt24hdz7 book      j.ctt24hd… NA         Authors’ … NA            <tibble> #>  5 j.ctt24hdz7 book      j.ctt24hd… 1.         The enigm… NA            <tibble> #>  6 j.ctt24hdz7 book      j.ctt24hd… 2.         ‘Anxiety,… Fiji’s road … <tibble> #>  7 j.ctt24hdz7 book      j.ctt24hd… 3.         Fiji’s De… Who, what, w… <tibble> #>  8 j.ctt24hdz7 book      j.ctt24hd… 4.         ‘This pro… The aftermat… <tibble> #>  9 j.ctt24hdz7 book      j.ctt24hd… 5.         The chang… NA            <tibble> #> 10 j.ctt24hdz7 book      j.ctt24hd… 6.         The Fiji … Analyzing th… <tibble> #> # ℹ 26 more rows #> # ℹ 2 more variables: abstract <chr>, part_first_page <chr>  tidyr::unnest(parts) #> Warning: `cols` is now required when using `unnest()`. #> ℹ Please use `cols = c(authors)`. #> # A tibble: 39 × 14 #>    book_id     file_name part_id      part_label part_title part_subtitle prefix #>    <chr>       <chr>     <chr>        <chr>      <chr>      <chr>         <chr>  #>  1 j.ctt24hdz7 book      j.ctt24hdz7… NA         Front Mat… NA            NA     #>  2 j.ctt24hdz7 book      j.ctt24hdz7… NA         Table of … NA            NA     #>  3 j.ctt24hdz7 book      j.ctt24hdz7… NA         Acronyms … NA            NA     #>  4 j.ctt24hdz7 book      j.ctt24hdz7… NA         Authors’ … NA            NA     #>  5 j.ctt24hdz7 book      j.ctt24hdz7… 1.         The enigm… NA            NA     #>  6 j.ctt24hdz7 book      j.ctt24hdz7… 1.         The enigm… NA            NA     #>  7 j.ctt24hdz7 book      j.ctt24hdz7… 2.         ‘Anxiety,… Fiji’s road … NA     #>  8 j.ctt24hdz7 book      j.ctt24hdz7… 3.         Fiji’s De… Who, what, w… NA     #>  9 j.ctt24hdz7 book      j.ctt24hdz7… 4.         ‘This pro… The aftermat… NA     #> 10 j.ctt24hdz7 book      j.ctt24hdz7… 5.         The chang… NA            NA     #> # ℹ 29 more rows #> # ℹ 7 more variables: given_name <chr>, surname <chr>, string_name <chr>, #> #   suffix <chr>, author_number <dbl>, abstract <chr>, part_first_page <chr>"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_file_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the basename of a path — jst_get_file_name","title":"Extract the basename of a path — jst_get_file_name","text":"helper simply extracts basename path removes extension, e.g. foo/bar.txt shortened bar.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_file_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the basename of a path — jst_get_file_name","text":"","code":"jst_get_file_name(file_path)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_file_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the basename of a path — jst_get_file_name","text":"file_path path file","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_file_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the basename of a path — jst_get_file_name","text":"character vector, containing basename file without extension.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_footnotes.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract all footnotes — jst_get_footnotes","title":"Extract all footnotes — jst_get_footnotes","text":"function extracts content fn-group journal-articles.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_footnotes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract all footnotes — jst_get_footnotes","text":"","code":"jst_get_footnotes(file_path)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_footnotes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract all footnotes — jst_get_footnotes","text":"file_path path .xml-file footnotes extracted.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_footnotes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract all footnotes — jst_get_footnotes","text":"tibble containing content fn-group (usually footnotes). footnotes, NA_character returned column footnotes.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_footnotes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract all footnotes — jst_get_footnotes","text":"fn-group usually contains footnotes corresponding article. However, since footnotes currently fully supported DfR, comprehensive documentation different variants. jstor therefore extracts content fn-group exactly appears data. , might content present footnotes. order get available information citation data, might need combine jst_get_footnotes() jst_get_references().","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_footnotes.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract all footnotes — jst_get_footnotes","text":"","code":"jst_get_footnotes(jst_example(\"article_with_footnotes.xml\")) #> # A tibble: 7 × 2 #>   file_name              footnotes                                               #>   <chr>                  <chr>                                                   #> 1 article_with_footnotes \"[Footnotes]\"                                           #> 2 article_with_footnotes \"9\\nQuarterly, vol. XIII, no. 1,\\nentries for April 19… #> 3 article_with_footnotes \"10\\nQuarterly, vol. XIII,\\nno. 1, p. 8.\"               #> 4 article_with_footnotes \"14\\nQuarterly, vol. VIII, no. 1.\\nOlympia Columbian, … #> 5 article_with_footnotes \"26\\nQuarterly, vol. XII,\\nno. 2, p. 141.\"              #> 6 article_with_footnotes \"32\\nDr. David S. Maynard, later (March 31, 1852)\"      #> 7 article_with_footnotes \"34\\nThomas Linklater, Shepherd, since October 6, 1849…"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_full_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Import full-text — jst_get_full_text","title":"Import full-text — jst_get_full_text","text":"function imports full_text contents JSTOR-article.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_full_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import full-text — jst_get_full_text","text":"","code":"jst_get_full_text(filename)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_full_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import full-text — jst_get_full_text","text":"filename path file.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_full_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Import full-text — jst_get_full_text","text":"tibble, containing file-path id, full content file, encoding used read .","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_journal_overview.html","id":null,"dir":"Reference","previous_headings":"","what":"Get table with information on journals — jst_get_journal_overview","title":"Get table with information on journals — jst_get_journal_overview","text":"Download recent display cached version data journals.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_journal_overview.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get table with information on journals — jst_get_journal_overview","text":"","code":"jst_get_journal_overview(most_recent = FALSE, quiet = FALSE)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_journal_overview.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get table with information on journals — jst_get_journal_overview","text":"most_recent recent version downloaded DfR? (Currently disabled due changes JSTOR-servers). quiet status messages download printed?","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_journal_overview.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get table with information on journals — jst_get_journal_overview","text":"tibble various information journals.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_journal_overview.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get table with information on journals — jst_get_journal_overview","text":"analysing sample articles DfR, might helpful context journals sample. function provides tibble various information like full name journal, short version name (sometimes referred JCODE), dates first last (available) issues published, etc. data journals might change. Therefore function provides two sources data: cached version gets updated every release, ability pull recent version directly DfR (temporarily disabled.) cached version updated 2020-04-03.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_journal_overview.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get table with information on journals — jst_get_journal_overview","text":"","code":"# use the function without arguments to get a tibble from disk jst_get_journal_overview() #> # A tibble: 4,359 × 13 #>    title  journal_id issn  eissn doi   url   discipline publisher coverage_range #>    <chr>  <chr>      <chr> <chr> <chr> <chr> <chr>      <chr>     <chr>          #>  1 14th … 14centeng… 0737… NA    10.2… http… Humanitie… Penn Sta… 1974-1983      #>  2 19th-… 19thcentu… 0148… 1533… 10.2… http… Arts ; Mu… Universi… 1977-2016      #>  3 291    291        1054… NA    10.2… http… Art & Art… Thomas J… 1915-1916      #>  4 4S Re… 4sreview   0738… NA    10.2… http… History ;… Sage Pub… 1972-1985      #>  5 AA Fi… aafiles    0261… NA    10.2… http… Architect… Architec… 1981-2017      #>  6 AAA: … aaaarbean… 0171… NA    10.2… http… American … Narr Fra… 1976-2018      #>  7 AATSE… aatseeljo… 2325… 2379… 10.2… http… Area Stud… American… 1945-1956      #>  8 AAUP … aaupbulle… 0001… NA    10.2… http… Education… American… 1915-1978      #>  9 A.A.V… aavnewsle… 2154… NA    10.2… http… Biologica… Associat… 1980-1986      #> 10 AAV T… aavtoday   0892… NA    10.2… http… Biologica… Associat… 1980-1988      #> # ℹ 4,349 more rows #> # ℹ 4 more variables: oclc_catalog_identifier <chr>, #> #   lccn_catalog_identifier <chr>, archive_release_date <chr>, #> #   collections <chr>  if (FALSE) { # download the most recent version from DfR jst_get_journal_overview(most_recent = TRUE) }"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_ngram.html","id":null,"dir":"Reference","previous_headings":"","what":"Read ngram data — jst_get_ngram","title":"Read ngram data — jst_get_ngram","text":"Read data ngrams via readr::read_tsv().","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_ngram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read ngram data — jst_get_ngram","text":"","code":"jst_get_ngram(file)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_ngram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read ngram data — jst_get_ngram","text":"file path file zip location jst_subset_ngrams().","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_ngram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read ngram data — jst_get_ngram","text":"tibble::tibble() two columns: ngram: ngram term (unigram, bigram, trigram) n: integer number times term occurred original file","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_ngram.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Read ngram data — jst_get_ngram","text":"function mainly useful used together jst_import_zip, can use specify reading ngram data.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_references.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract all references — jst_get_references","title":"Extract all references — jst_get_references","text":"function extracts content ref-list xml-file.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_references.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract all references — jst_get_references","text":"","code":"jst_get_references(file_path, parse_refs = FALSE)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_references.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract all references — jst_get_references","text":"file_path path .xml-file references extracted. parse_refs references parsed, available?","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_references.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract all references — jst_get_references","text":"tibble following columns: file_name: identifier article references come . ref_title: title references sections. ref_authors: string authors. Several authors separated ;. ref_editors: string editors, available. ref_collab: field may contain information authors, authors available. ref_item_title: title cited entry. books often empty, title ref_source. ref_year: year, often article's publication year, always. ref_source: source cited entry. books often title book, articles publisher journal. ref_volume: volume journal article. ref_first_page: first page article/chapter. ref_last_page: last page article/chapter. ref_publisher: books publisher, articles often missing. ref_publication_type: Known types: book, journal, web, . ref_unparsed: full references entry unparsed form.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_references.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract all references — jst_get_references","text":"content may contain references endnotes, depending article used citations. Since references currently fully supported DfR, comprehensive documentation different variants. jstor therefore extracts content ref-list exactly appears data. , might content present references. order get available information citation data, might need combine jst_get_references() jst_get_footnotes(). newer xml-files, option extract single elements like authors, title date source, yet implemented. general, implementation fast jst_get_article() - articles many references slow process .","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_references.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract all references — jst_get_references","text":"","code":"jst_get_references(jst_example(\"article_with_references.xml\")) #> # A tibble: 23 × 13 #>    file_name ref_title ref_authors ref_collab ref_item_title ref_year ref_source #>    <chr>     <chr>     <chr>       <chr>      <chr>          <chr>    <chr>      #>  1 article_… Bibliogr… NA          NA         NA             NA       NA         #>  2 article_… Bibliogr… NA          NA         NA             NA       NA         #>  3 article_… Referenc… NA          NA         NA             NA       NA         #>  4 article_… Referenc… NA          NA         NA             NA       NA         #>  5 article_… Referenc… NA          NA         NA             NA       NA         #>  6 article_… Referenc… NA          NA         NA             NA       NA         #>  7 article_… Referenc… NA          NA         NA             NA       NA         #>  8 article_… Referenc… NA          NA         NA             NA       NA         #>  9 article_… Referenc… NA          NA         NA             NA       NA         #> 10 article_… Referenc… NA          NA         NA             NA       NA         #> # ℹ 13 more rows #> # ℹ 6 more variables: ref_volume <chr>, ref_first_page <chr>, #> #   ref_last_page <chr>, ref_publisher <chr>, ref_publication_type <chr>, #> #   ref_unparsed <chr>  # import parsed references jst_get_references(   jst_example(\"parsed_references.xml\"),   parse_refs = TRUE )  #> # A tibble: 7 × 14 #>   file_name ref_title ref_authors ref_editors ref_collab ref_item_title ref_year #>   <chr>     <chr>     <chr>       <chr>       <chr>      <chr>          <chr>    #> 1 parsed_r… Notes     NA          NA          NA         NA             2005     #> 2 parsed_r… Referenc… Acohido, B… NA          NA         “Snowden Case… 2013     #> 3 parsed_r… Referenc… NA          NA          Amnesty I… NA             2013     #> 4 parsed_r… Referenc… Jacobson, … D. E. Davi… NA         Chapter title  2009     #> 5 parsed_r… Referenc… Costall, A… NA          NA         “Some article… 1980     #> 6 parsed_r… Referenc… Hudson, W.  NA          NA         Another artic… 2000     #> 7 parsed_r… Referenc… Fries-Brit… NA          NA         Some article … 2000     #> # ℹ 7 more variables: ref_source <chr>, ref_volume <chr>, ref_first_page <chr>, #> #   ref_last_page <chr>, ref_publisher <chr>, ref_publication_type <chr>, #> #   ref_unparsed <chr>"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_total_pages.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate total pages — jst_get_total_pages","title":"Calculate total pages — jst_get_total_pages","text":"function simple helper calculate total number pages article.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_total_pages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate total pages — jst_get_total_pages","text":"","code":"jst_get_total_pages(first_page, last_page, page_range, quietly = FALSE)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_total_pages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate total pages — jst_get_total_pages","text":"first_page first page article (numeric). last_page last page article (numeric). page_range page range article (character). quietly Sometimes page ranges contain roman numerals like xiv. recognized, return NA raise warning. set TRUE, warning raised.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_total_pages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate total pages — jst_get_total_pages","text":"vector calculated total pages.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_total_pages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate total pages — jst_get_total_pages","text":"function deals four cases: three arguments missing, NA returned. page_range supplied, number pages calculated . first page supplied, NA returned. first last page supplied, number pages calculated last_page - first_page + 1. algorithm parse page ranges works follows: typical page range 1-10, 200 article starts page 1, ends page 10, erratum page 200. case, range calculated range + single_page, (10 - 1 + 1) + 1 = 11. Sometimes multiple ranges given: 1-10, 11-20. cases ranges summed: (10 - 1 + 1) + (20 - 11 + 1) = 20. Another specification multiple ranges 1-10+11-20, treated similarly.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_get_total_pages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate total pages — jst_get_total_pages","text":"","code":"# calculate pages from first and last page first_pages <- sample(30:50, 10) last_pages <- first_pages + sample(5:20, 10) page_ranges <- rep(NA_character_, 10)  jst_get_total_pages(first_pages, last_pages, page_ranges) #>  [1] 10 21 13 18  7 12 20 15  9 14  # get pages from page range jst_get_total_pages(NA_real_, NA_real_, \"51 - 70\") #> [1] 20 jst_get_total_pages(NA_real_, NA_real_, \"51 - 70, 350\") #> [1] 21 jst_get_total_pages(NA_real_, NA_real_, \"350, 51 - 70\") #> [1] 21 jst_get_total_pages(NA_real_, NA_real_, \"51 - 70, 80-100\") #> [1] 41 jst_get_total_pages(NA_real_, NA_real_, \"51-70+350\") #> [1] 21"},{"path":"https://docs.ropensci.org/jstor/reference/jst_import.html","id":null,"dir":"Reference","previous_headings":"","what":"Wrapper for file import — jst_import","title":"Wrapper for file import — jst_import","text":"function applies import function list xml-files .zip-archive case jst_import_zip saves output batches .csv-files disk.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_import.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wrapper for file import — jst_import","text":"","code":"jst_import(   in_paths,   out_file,   out_path = NULL,   .f,   col_names = TRUE,   n_batches = NULL,   files_per_batch = NULL,   show_progress = TRUE )  jst_import_zip(   zip_archive,   import_spec,   out_file,   out_path = NULL,   col_names = TRUE,   n_batches = NULL,   files_per_batch = NULL,   show_progress = TRUE,   rows = NULL )"},{"path":"https://docs.ropensci.org/jstor/reference/jst_import.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wrapper for file import — jst_import","text":"in_paths character vector xml-files imported out_file Name files export . batch gets appended increasing number. out_path Path export files (combined filename). .f Function use import. Can one jst_get_article, jst_get_authors, jst_get_references, jst_get_footnotes, jst_get_book jst_get_chapter. col_names column names written file? Defaults TRUE. n_batches Number batches, defaults 1. files_per_batch Number files batch. Can used instead n_batches, conjunction. show_progress Displays progress bar batch, session interactive. zip_archive path .zip-archive DfR import_spec specification jst_define_import parts .zip-archive imported via functions. rows Mainly used testing, decrease number files imported (.e. 1:100).","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_import.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Wrapper for file import — jst_import","text":"Writes .csv-files disk.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_import.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Wrapper for file import — jst_import","text":"Along way, wrap three functions, make process converting many files easier: purrr::safely() furrr::future_map() readr::write_csv() using one find_* functions, usually errors. avoid whole computation fail unlikely event error occurs, use safely() us continue process, catch error along way. many files import, might benefit executing function parallel. use futures give maximum flexibility. default code executed sequentially. want run parallel, simply call future::plan() future::multisession() argument running jst_import jst_import_zip. importing files, written disk readr::write_csv(). Since might run memory importing large quantity files, can split files import  batches. batch treated separately, therefore batch multiple processes future::multisession() spawned, added plan. reason, recommended small batches, overhead starting ending processes. hand, batches large, exceed memory limitations. value 10000 20000 files_per_batch work fine machines. session interactive show_progress TRUE, progress bar displayed batch.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/jstor/reference/jst_import.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Wrapper for file import — jst_import","text":"","code":"if (FALSE) { # read from file list -------- # find all files meta_files <- list.files(pattern = \"xml\", full.names = TRUE)  # import them via `jst_get_article` jst_import(meta_files, out_file = \"imported_metadata\", .f = jst_get_article,            files_per_batch = 25000)             # do the same, but in parallel library(future) plan(multiprocess) jst_import(meta_files, out_file = \"imported_metadata\", .f = jst_get_article,            files_per_batch = 25000)  # read from zip archive ------  # define imports imports <- jst_define_import(article = c(jst_get_article, jst_get_authors))  # convert the files to .csv jst_import_zip(\"my_archive.zip\", out_file = \"my_out_file\",                   import_spec = imports) }"},{"path":"https://docs.ropensci.org/jstor/reference/jst_preview_zip.html","id":null,"dir":"Reference","previous_headings":"","what":"Preview content of zip files — jst_preview_zip","title":"Preview content of zip files — jst_preview_zip","text":"function gives quick preview .zip-file DfR contains.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_preview_zip.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preview content of zip files — jst_preview_zip","text":"","code":"jst_preview_zip(zip_archive)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_preview_zip.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preview content of zip files — jst_preview_zip","text":"zip_archive path .zip-file DfR","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_preview_zip.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preview content of zip files — jst_preview_zip","text":"function returns tibble three columns: type: metadata form ngram meta_type: type metadata (book_chapter, journal article, ...) n: count category","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_preview_zip.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preview content of zip files — jst_preview_zip","text":"","code":"jst_preview_zip(jst_example(\"pseudo_dfr.zip\")) #> # A tibble: 4 × 3 #>   type     meta_type           n #>   <chr>    <chr>           <int> #> 1 metadata book_chapter        1 #> 2 metadata journal_article     1 #> 3 metadata pamphlet            1 #> 4 ngram1   ngram1              1"},{"path":"https://docs.ropensci.org/jstor/reference/jst_re_import.html","id":null,"dir":"Reference","previous_headings":"","what":"Re-import files — jst_re_import","title":"Re-import files — jst_re_import","text":"jst_re_import() lets re-import file exported via jst_import() jst_import_zip().","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_re_import.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Re-import files — jst_re_import","text":"","code":"jst_re_import(file, warn = TRUE)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_re_import.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Re-import files — jst_re_import","text":"file path .csv file. warn warnings emitted, type file determined?","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_re_import.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Re-import files — jst_re_import","text":"tibble, columns determined based heuristics applied input file.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_re_import.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Re-import files — jst_re_import","text":"attempting re-import, heuristic applied. file column names match names find_* functions, file read corresponding specifications. column names recognized, files recognized based number columns. Since references footnotes two columns, first line inspected either \"Referenc...|Bilbio...|Endnote...\" \"Footnote...\". case still match, file read readr::read_csv() guess_max = 5000 warning raised.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/jstor/reference/jst_subset_ngrams.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a subset of ngrams — jst_subset_ngrams","title":"Define a subset of ngrams — jst_subset_ngrams","text":"function helps defining subset ngram files imported, since importing ngrams can expensive (terms cpu memory).","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_subset_ngrams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a subset of ngrams — jst_subset_ngrams","text":"","code":"jst_subset_ngrams(zip_archives, ngram_type, selection, by = file_name)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_subset_ngrams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a subset of ngrams — jst_subset_ngrams","text":"zip_archives character vector one multiple zip-files. ngram_type One \"ngram1\", \"ngram2\" \"ngram3\" selection data.frame articles/books selected. column name matching.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_subset_ngrams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a subset of ngrams — jst_subset_ngrams","text":"list zip-locations can read via jst_get_ngram().","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_subset_ngrams.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Define a subset of ngrams — jst_subset_ngrams","text":"","code":"# create sample output tmp <- tempdir() jst_import_zip(jst_example(\"pseudo_dfr.zip\"),                import_spec = jst_define_import(book = jst_get_book),                out_file = \"test\", out_path = tmp) #> Processing files for book_chapter with functions jst_get_book  # re-import as our selection for which we would like to import ngrams selection <- jst_re_import(file.path(tmp,                                       \"test_book_chapter_jst_get_book-1.csv\"))  # get location of file zip_loc <- jst_subset_ngrams(jst_example(\"pseudo_dfr.zip\"), \"ngram1\",                              selection)   # import ngram jst_get_ngram(zip_loc[[1]]) #> # A tibble: 2 × 3 #>   file_name                  ngram        n #>   <chr>                      <chr>    <int> #> 1 book-chapter-standard_book Common     400 #> 2 book-chapter-standard_book Uncommon     5 unlink(tmp)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_unify_journal_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Unify journal IDs — jst_unify_journal_id","title":"Unify journal IDs — jst_unify_journal_id","text":"function simple wrapper unify journal ids.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_unify_journal_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unify journal IDs — jst_unify_journal_id","text":"","code":"jst_unify_journal_id(meta_data, remove_cols = TRUE)"},{"path":"https://docs.ropensci.org/jstor/reference/jst_unify_journal_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unify journal IDs — jst_unify_journal_id","text":"meta_data Data processed via jst_get_article(). remove_cols original columns removed unifying?","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_unify_journal_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unify journal IDs — jst_unify_journal_id","text":"modified tibble. modified tibble.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_unify_journal_id.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Unify journal IDs — jst_unify_journal_id","text":"Date journal ids can found three columns: journal_pub_id, journal_jcode journal_doi. experience, time relevant dat ais present journal_pub_id journal_jcode, journal_jcode common identifier. function takes value journal_pub_id, missing, journal_jcode. journal_doi currently disregarded.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jst_unify_journal_id.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unify journal IDs — jst_unify_journal_id","text":"","code":"article <- jst_get_article(jst_example(\"article_with_references.xml\"))  jst_unify_journal_id(article) #> # A tibble: 1 × 17 #>   file_name  journal_title article_doi article_pub_id article_jcode article_type #>   <chr>      <chr>         <chr>       <chr>          <chr>         <chr>        #> 1 article_w… Transactions… 10.2307/32… NA             NA            research-ar… #> # ℹ 11 more variables: article_title <chr>, volume <chr>, issue <chr>, #> #   language <chr>, pub_day <chr>, pub_month <chr>, pub_year <int>, #> #   first_page <chr>, last_page <chr>, page_range <chr>, journal_id <chr>   # per default, original columns with data on the journal are removed library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union  jst_unify_journal_id(article) %>%    select(contains(\"journal\")) %>%    names() #> [1] \"journal_title\" \"journal_id\"       # you can keep them by setting `remove_cols` to `FALSE` jst_unify_journal_id(article, remove_cols = FALSE) %>%     select(contains(\"journal\")) %>%   names() #> [1] \"journal_doi\"    \"journal_jcode\"  \"journal_pub_id\" \"journal_title\"  #> [5] \"journal_id\""},{"path":"https://docs.ropensci.org/jstor/reference/jstor.html","id":null,"dir":"Reference","previous_headings":"","what":"jstor: Read Data from JSTOR/DfR — jstor","title":"jstor: Read Data from JSTOR/DfR — jstor","text":"tool Data Research (DfR) JSTOR valuable source citation analysis text mining. jstor provides functions suggests workflows importing datasets DfR.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jstor.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"jstor: Read Data from JSTOR/DfR — jstor","text":"Please refer vignettes information use package: browseVignettes(\"jstor\") encounter issues ideas new features, please file issue https://github.com/ropensci/jstor/issues.","code":""},{"path":"https://docs.ropensci.org/jstor/reference/jstor.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"jstor: Read Data from JSTOR/DfR — jstor","text":"Thomas Klebel","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-0311","dir":"Changelog","previous_headings":"","what":"jstor 0.3.11","title":"jstor 0.3.11","text":"CRAN release: 2023-08-16 small release fix compatibility future v1.33.0.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-0310","dir":"Changelog","previous_headings":"","what":"jstor 0.3.10","title":"jstor 0.3.10","text":"CRAN release: 2021-12-08 small release fix compatibility readr v2.0.0. addition, various minor improvements made across package get back CRAN.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-039","dir":"Changelog","previous_headings":"","what":"jstor 0.3.9","title":"jstor 0.3.9","text":"CRAN release: 2020-06-04 small release fix compatibility dplyr v1.0.0.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-038","dir":"Changelog","previous_headings":"","what":"jstor 0.3.8","title":"jstor 0.3.8","text":"CRAN release: 2020-04-03 package homepage switched https://ropensci.github.io/jstor https://docs.ropensci.org/jstor. simplifies building site aligns rest rOpenSci’s packages. #80 Compatibility fix tibble 3.0.0.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-037","dir":"Changelog","previous_headings":"","what":"jstor 0.3.7","title":"jstor 0.3.7","text":"CRAN release: 2019-09-05 small release fix compatibility tidyr v1.0.0. Furthermore, formerly defunct functions following old naming conventions (like find_article(), find_references(), etc.) removed.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-036","dir":"Changelog","previous_headings":"","what":"jstor 0.3.6","title":"jstor 0.3.6","text":"CRAN release: 2018-12-12 another small release fix compatibility readr v1.3.0 tibble v2.0.0. changes.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-035","dir":"Changelog","previous_headings":"","what":"jstor 0.3.5","title":"jstor 0.3.5","text":"CRAN release: 2018-11-25 small release, mainly fix compatibility version 1.2.0 readr. one breaking change however:","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"breaking-changes-0-3-5","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"jstor 0.3.5","text":"Output column names jst_get_refernces renamed avoid ambiguity matching output jst_get_article. columns now ref_* prefix.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-034","dir":"Changelog","previous_headings":"","what":"jstor 0.3.4","title":"jstor 0.3.4","text":"CRAN release: 2018-10-17 Added option parse references, information available. #32 Output error messages “re-imported” properly well. Example files renamed clarity. sample_ replaced article_ removed altogether. Reworked internals catching misspecifications jst_define_import due upcoming release rlang v0.3.0.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-033","dir":"Changelog","previous_headings":"","what":"jstor 0.3.3","title":"jstor 0.3.3","text":"CRAN release: 2018-09-30","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"removed-functionality-0-3-3","dir":"Changelog","previous_headings":"","what":"Removed functionality","title":"jstor 0.3.3","text":"option download recent data journals directly JSTOR (jst_get_journal_overview(most_recent = T)) removed due changes server. try find solution JSTOR support can add functionality .","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"new-features-0-3-3","dir":"Changelog","previous_headings":"","what":"New features","title":"jstor 0.3.3","text":"jst_define_import now prints specification pretty informative way. jst_define_import now checks definition extensively: jst_define_import(article = jst_get_book) similar mis-specifications raise error. Import crayon package colourful error messages places, easier read.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"bug-fixes-0-3-3","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"jstor 0.3.3","text":"Removed outdated function vignette batch importing files.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"other-changes-0-3-3","dir":"Changelog","previous_headings":"","what":"Other changes","title":"jstor 0.3.3","text":"old group find_* functions now defunct (raise error). Updated cached version journal data.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-032","dir":"Changelog","previous_headings":"","what":"jstor 0.3.2","title":"jstor 0.3.2","text":"CRAN release: 2018-07-20 hotfix resolve issue writing directories temporary folders tests, happend first place.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-031","dir":"Changelog","previous_headings":"","what":"jstor 0.3.1","title":"jstor 0.3.1","text":"CRAN release: 2018-07-13 jstor now part rOpenSci. removed arguments new_col jst_unify_journal_id jst_add_total_pages, since built dev version rlang. version CRAN, re-introduced. fixed issues README man files regarding changes introduced 0.3.0.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"breaking-changes-0-3-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"jstor 0.3.0","text":"jst_import jst_import_zip now use futures backend parallel processing. makes internals compact reduces dependencies. Furthermore reduces number arguments, since argument cores removed. default, functions run sequentially. want execute parallel, use futures: want terminate proceses, least *nix-systems need kill manually (). functions renamed use unified naming scheme: jst_*. former group find_* functions now called jst_get_*, jst_get_article(). previous functions deprecated removed submission CRAN. unique identifier matching across files renamed file_name, corresponding helper get file name get_basename jst_get_file_name.","code":"library(future) plan(multiprocess)  jst_import_zip(\"zip-archive.zip\",                import_spec = jst_define_import(article = jst_get_article),                out_file = \"outfile\")"},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"importing-data-directly-from-zip-files-0-3-0","dir":"Changelog","previous_headings":"","what":"Importing data directly from zip-files","title":"jstor 0.3.0","text":"new set functions lets directly import files .zip-archives: jst_import_zip() jst_define_import(). following example, zip-archive DfR want import metadata books articles. articles want apply jst_get_article() jst_get_authors(), books jst_get_book(), want read unigrams (ngram1). First specify want, apply zip-archive: archive contains also research reports, pamphlets ngrams, imported. however change specification, wanted import kinds ngrams (given originally requested DfR): Note however larger archives, importing ngrams takes long time. thus advisable import ngrams articles want analyse, .e. likely subset initial request. new function jst_subset_ngrams() helps (see also section importing bigrams case study. importing files zip-archive, can get quick overview jst_preview_zip().","code":"# specify definition import_spec <- jst_define_import(article = c(jst_get_article, jst_get_authors),                                  book = jst_get_book,                                  ngram1 = jst_get_ngram)  # apply definition to archive jst_import_zip(\"zip_archive.zip\",                import_spec = import_spec,                out_file = \"out_path\") # import multiple forms of ngrams import_spec <- jst_define_import(article = c(jst_get_article, jst_get_authors),                                  book = jst_get_book,                                  ngram1 = jst_get_ngram,                                  ngram2 = jst_get_ngram,                                  ngram3 = jst_get_ngram)"},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"new-vignette-0-3-0","dir":"Changelog","previous_headings":"","what":"New vignette","title":"jstor 0.3.0","text":"new vignette(\"known-quirks\") lists common problems data JSTOR/DfR. Contributions cases welcome!","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"new-functions-0-3-0","dir":"Changelog","previous_headings":"","what":"New functions","title":"jstor 0.3.0","text":"New function jst_get_journal_overview() supplies tibble contextual information journals JSTOR. New function jst_combine_outputs() applies jst_re_import() whole directory lets combine related files one go. uses file structure jst_import() jst_import_zip() provide heuristic: filename dash one multiple digits end (filename-1.csv). files identical names (disregarding dash digits) combined one file. New function jst_re_import() lets re_import .csv file jstor_import() jst_import_zip() exported. tries guess type content based column names , column names available, number columns, raising warning guessing fails reverting generic import. new function jst_subset_ngrams() lets create subset ngram files within zip-file can import jst_get_ngram(). new set convenience functions taking cleaning steps: jst_clean_page() tries turn character vector pages numeric one, jst_unify_journal_id() merges different specifications journals one, jst_add_total_pages() adds total count pages per article, jst_augment() calls three functions clean data set one go.","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"minor-changes-0-3-0","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"jstor 0.3.0","text":"Improved documentation regarding endnotes (thanks @elinw) jst_import jst_import_zip new argument: n_batches lets specify number batches directly","code":""},{"path":"https://docs.ropensci.org/jstor/news/index.html","id":"jstor-026","dir":"Changelog","previous_headings":"","what":"jstor 0.2.6","title":"jstor 0.2.6","text":"added lengthy case study https://tklebel.github.io/jstor/articles/analysing-n-grams.html added pkgdown site https://tklebel.github.io/jstor/ changed implementation parallel execution jstor_import parallel::mclapply foreach::foreach snow backend %dopar%. added support progress bars #34 jstor_import now writes column names default #29 new helper get_basename helps get basename file without extension find_article coerce days months integer , since might information stored text. Added NEWS.md file track changes package.","code":""}]
